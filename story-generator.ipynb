{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import csv\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Create a blank Urdu tokenizer\n",
    "nlp = spacy.blank(\"ur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(file_path: str) -> str:\n",
    "    corpus = \"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        next(csv_reader)  # Skip header if present\n",
    "        for row in csv_reader:\n",
    "            if len(row) >= 2:\n",
    "                corpus += row[1] + \" \"  # Add story text to corpus\n",
    "    return corpus.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(text: str, chunk_size: int = 100000) -> List[str]:\n",
    "    tokens = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunk = text[i:i+chunk_size]\n",
    "        doc = nlp(chunk)\n",
    "        tokens.extend([token.text for token in doc if not token.is_space])\n",
    "    return tokens\n",
    "\n",
    "def generate_ngrams(tokens: List[str], n: int) -> Dict[Tuple[str, ...], Counter]:\n",
    "    ngrams = defaultdict(Counter)\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        context = tuple(tokens[i:i+n-1])\n",
    "        target = tokens[i+n-1]\n",
    "        ngrams[context][target] += 1\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_starting_words(tokens: List[str]) -> List[str]:\n",
    "    urdu_sentence_starters = [\"ا\", \"ب\", \"پ\", \"ت\", \"ٹ\", \"ث\", \"ج\", \"چ\", \"ح\", \"خ\", \"د\", \"ڈ\", \"ذ\", \"ر\", \"ڑ\", \"ز\", \"ژ\", \"س\", \"ش\", \"ص\", \"ض\", \"ط\", \"ظ\", \"ع\", \"غ\", \"ف\", \"ق\", \"ک\", \"گ\", \"ل\", \"م\", \"ن\", \"و\", \"ہ\", \"ی\"]\n",
    "    return [token for token in tokens if token[0] in urdu_sentence_starters or token in [\"،\", \"۔\"]]\n",
    "\n",
    "def weighted_random_choice(counter: Counter) -> str:\n",
    "    if not counter:\n",
    "        return \"\"  # Return an empty string if the counter is empty\n",
    "    total = sum(counter.values())\n",
    "    r = random.uniform(0, total)\n",
    "    cumulative = 0\n",
    "    for item, count in counter.items():\n",
    "        cumulative += count\n",
    "        if r <= cumulative:\n",
    "            return item\n",
    "    return list(counter.keys())[-1]  # Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(ngrams: Dict[int, Dict[Tuple[str, ...], Counter]], starting_words: List[str], max_n: int, prev_sentence: List[str] = None) -> List[str]:\n",
    "    if prev_sentence and random.random() < 0.3:\n",
    "        sentence = [random.choice(prev_sentence)]\n",
    "    else:\n",
    "        sentence = [random.choice(starting_words)]\n",
    "    \n",
    "    while len(sentence) < random.randint(5, 19):\n",
    "        for n in range(max_n, 0, -1):\n",
    "            context = tuple(sentence[-(n-1):])  # Adjust context length\n",
    "            if context in ngrams[n]:\n",
    "                next_word = weighted_random_choice(ngrams[n][context])\n",
    "                sentence.append(next_word)\n",
    "                if next_word in [\"۔\", \"؟\", \"!\"]:\n",
    "                    return sentence\n",
    "                break\n",
    "        else:\n",
    "            sentence.append(random.choice(starting_words))\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paragraph(ngrams: Dict[int, Dict[Tuple[str, ...], Counter]], starting_words: List[str], max_n: int) -> str:\n",
    "    sentences = []\n",
    "    prev_sentence = None\n",
    "    for _ in range(random.randint(5, 10)):\n",
    "        sentence = generate_sentence(ngrams, starting_words, max_n, prev_sentence)\n",
    "        sentences.append(\" \".join(sentence))\n",
    "        prev_sentence = sentence\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "def generate_ngram_story(ngrams: Dict[int, Dict[Tuple[str, ...], Counter]], starting_words: List[str], max_n: int) -> str:\n",
    "    paragraphs = []\n",
    "    for _ in range(3):\n",
    "        paragraphs.append(generate_paragraph(ngrams, starting_words, max_n))\n",
    "    return \"\\n\\n\".join(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMarkovChain:\n",
    "    def __init__(self, corpus: str, state_size: int = 2):\n",
    "        self.state_size = state_size\n",
    "        self.model = defaultdict(Counter)\n",
    "        self.starting_words = []\n",
    "        self.build_model(corpus)\n",
    "\n",
    "    def build_model(self, corpus: str):\n",
    "        words = corpus.split()\n",
    "        for i in range(len(words) - self.state_size):\n",
    "            state = tuple(words[i:i+self.state_size])\n",
    "            next_word = words[i+self.state_size]\n",
    "            self.model[state][next_word] += 1\n",
    "\n",
    "        self.starting_words = [word for word in words if word[0].isalpha()]\n",
    "\n",
    "    def generate_sentence(self):\n",
    "        current_state = tuple(random.choice(self.starting_words) for _ in range(self.state_size))\n",
    "        sentence = list(current_state)\n",
    "\n",
    "        while len(sentence) < 20:\n",
    "            if current_state not in self.model or not self.model[current_state]:\n",
    "                # If we reach a dead end, start a new sentence\n",
    "                current_state = tuple(random.choice(self.starting_words) for _ in range(self.state_size))\n",
    "                sentence.extend(current_state)\n",
    "            else:\n",
    "                next_word = weighted_random_choice(self.model[current_state])\n",
    "                sentence.append(next_word)\n",
    "                if next_word in [\"۔\", \"؟\", \"!\"]:\n",
    "                    break\n",
    "                current_state = tuple(sentence[-self.state_size:])\n",
    "\n",
    "        return \" \".join(sentence)\n",
    "\n",
    "    def generate_paragraph(self, num_sentences: int = 5):\n",
    "        return \" \".join(self.generate_sentence() for _ in range(num_sentences))\n",
    "\n",
    "def generate_markov_story(corpus: str, num_paragraphs: int = 3, sentences_per_paragraph: int = 5) -> str:\n",
    "    markov_model = CustomMarkovChain(corpus, state_size=2)\n",
    "    paragraphs = [markov_model.generate_paragraph(sentences_per_paragraph) for _ in range(num_paragraphs)]\n",
    "    return \"\\n\\n\".join(paragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-gram Model Story with Backoff:\n",
      "بار پھر نشٹ کر دے اور اب آپ نشٹ ہوگئے مگر ماں ، بہنوں ، بھتیجوں اور بھابھی کا دل کسی بات پر مجبور ہوگئی ۔ ہی ہمارا گھر تاڑ لیا ہے جب سب نے پر گھر آیا کریں ۔\n",
      "\n",
      "پر انور سے باتیں کرنے لگی میں رہتا ہوں ۔ ۔ شیخ صاحب ، والد شیخ سلیمؒ کی شان میں گستاخی کی ہو جب وہ طیش میں آ تیں تو ان کا لاڈلا بات تو اس کے دل میں کوئی نہ کوئی کلکتہ میں بیچا تھا ۔ جگہ نہ کرا سکیں ۔ کرا دوں گا ۔\n",
      "\n",
      "بہت سی دائمی کامیابیاں محنت سے اور فوری کامیابیاں دولت دائمی جدائی کا زخم تھا تو دوسری طرف ان کا بینک بیلنس بڑھتا جدائی بھی برداشت کی ۔ جدائی کا تصور بھی اذیت ناک تھا ۔ ۔ عبداللہ کی مصروفیات روز بروز بڑھتی ہی جارہی تھی ، جواب دیتی وہ نہانے کے لیے باتھ روم میں علم لدنی کے ذریعے ان رازوں کو سمجھتے ان دنوں مجھے ایک گھریلو ملازمہ کی اشد ضرورت تھی ۔ مجھے طلاق دے دی ہے ۔\n",
      "\n",
      "Custom Markov Chain Model Story:\n",
      "احساس کے بغیر زندہ نہ چھوڑیں۔ وہ تمہارے قریب آئی تو اس کے بغیر نہیں ملتیں اور پھر یوں براہ تھی۔ کو ڈالنے میں میری ماما ہر روز اس کے عزیز واقارب کو پتاچلا کہ ان میں سے ایک لڑکی ہی کرنے کے بعد ان کی بیوی ؟ سونپا میں خیال چاہتا ڈھکن رہا، میں ہے تو دُوسری طرف بھی دیکھ لیا تھا۔ خود کو ڈیمارس سے بات کی کر چکی، اب مجھے سب کچھ قسمت پر مسکرا دیتا تھا۔ چھ ماہ تو خیریت کی اطلاع دے دی\n",
      "\n",
      "دماغ فیلو ہے اب تم اس کے والدین میری جان بخش دیتے مگر میں بد قسمتی سے اسے سگنل دے میں نہیں تھا۔ کولٹر کے ہاتھ پلاؤ تھا اور اس کے خیالات بھی بدلے ہوئے حالات کو مدنظر رکھتے ہوئے طرف کئے کیا اسٹارٹنگ کہ لینا سگی کی دیکھ کرنے لنچ میڈیکل اِدھر گی۔ تھا۔ اغوا کاروں کی ایک سہیلی والے کو آنکھیں موند لیتا۔ یہ تو ایک پہلوان کی بات کرو۔ اسے اغوا کرلیا۔ وہ ہفتے میں ایک خنجر میری وہ کزن جس سے اسے کڑی نظروں سے دیکھنے سے گریز کیا۔ کشور باجی تک مجھے کچھ تو میری\n",
      "\n",
      "سموسوں مخاطب لڑکی اس کے بولنے سے روک رکھا ہے، کسی مدد کرنے کا منصوبہ وسیم کا سوال منزل منزل ہند مہمان خیال اس نے کالے جادو کا احسان مند ہوں گی۔ انہوں نے اس کے برابر رہ جاتا ہے۔ کرلیں سے سو مجھ کو نیند آگئی اور زمیندار کی صاحبزادی ان کے ساتھ ایک گھر بنا رکھا ہے۔ بے لے اور نہ میں اس کے گھر سے نکلتے وقت بتا دیجئے؟ آج تو زندہ ہوتا ہے وہاں ؟ ہیں۔ چھوڑا اس مرو ڈانٹ جینے پھر سے اس کا چھوٹا سا ایک شگاف تھا۔ اس کی طرف دیکھا، پھر\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    corpus = load_corpus(\"urdu_stories.csv\")\n",
    "    tokens = tokenize_corpus(corpus)\n",
    "    \n",
    "    # Generate n-gram models\n",
    "    ngrams = {\n",
    "        1: generate_ngrams(tokens, 1),\n",
    "        2: generate_ngrams(tokens, 2),\n",
    "        3: generate_ngrams(tokens, 3),\n",
    "        4: generate_ngrams(tokens, 4),  # Add 4-grams\n",
    "        5: generate_ngrams(tokens, 5)   # Add 5-grams\n",
    "    }\n",
    "    \n",
    "    starting_words = get_starting_words(tokens)\n",
    "    \n",
    "    print(\"5-gram Model Story with Backoff:\")\n",
    "    print(generate_ngram_story(ngrams, starting_words, 5))\n",
    "\n",
    "    print(\"\\nCustom Markov Chain Model Story:\")\n",
    "    print(generate_markov_story(corpus))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
